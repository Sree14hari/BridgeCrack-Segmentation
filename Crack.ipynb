{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3b7a72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting 20000 images based on Real-World Width (mm)...\n",
      "Using conversion: 1 Pixel = 0.05 mm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [03:23<00:00, 98.15it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sorting Complete ---\n",
      "Light  (<= 0.1mm): 3039\n",
      "Medium (0.1-0.3mm): 1774\n",
      "Heavy  (> 0.3mm):  15187\n",
      "Check folder: Crack\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CRITICAL CONFIGURATION ---\n",
    "# You MUST adjust this value based on your camera distance!\n",
    "# Example: If a 10mm tape covers 200 pixels in your image, ratio is 10/200 = 0.05\n",
    "MM_PER_PIXEL = 0.05  \n",
    "\n",
    "def sort_cracks_by_mm(input_folder, output_base):\n",
    "    # Define the new classes based on your screenshot\n",
    "    # We use 'heavy' instead of 'large' to match your standard\n",
    "    classes = ['light', 'medium', 'heavy']\n",
    "    for c in classes:\n",
    "        os.makedirs(os.path.join(output_base, c), exist_ok=True)\n",
    "        \n",
    "    valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp')\n",
    "    files = [f for f in os.listdir(input_folder) if f.lower().endswith(valid_extensions)]\n",
    "    \n",
    "    print(f\"Sorting {len(files)} images based on Real-World Width (mm)...\")\n",
    "    print(f\"Using conversion: 1 Pixel = {MM_PER_PIXEL} mm\")\n",
    "    \n",
    "    count_light = 0\n",
    "    count_medium = 0\n",
    "    count_heavy = 0\n",
    "    \n",
    "    for filename in tqdm(files):\n",
    "        img_path = os.path.join(input_folder, filename)\n",
    "        \n",
    "        # 1. Read Image (Grayscale)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None: continue\n",
    "        \n",
    "        # 2. Extract Crack (Thresholding)\n",
    "        # Inverts image so crack is WHITE, background is BLACK\n",
    "        # Note: If your preprocessing (Phase 1) was good, cracks are dark.\n",
    "        _, binary = cv2.threshold(img, 60, 255, cv2.THRESH_BINARY_INV)\n",
    "        \n",
    "        # 3. Clean Noise (Morphology)\n",
    "        # Remove tiny specs that are smaller than a real crack\n",
    "        kernel = np.ones((3,3), np.uint8)\n",
    "        cleaned = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)\n",
    "        \n",
    "        # 4. Measure Max Width in Pixels\n",
    "        dist_transform = cv2.distanceTransform(cleaned, cv2.DIST_L2, 5)\n",
    "        \n",
    "        if dist_transform.max() == 0:\n",
    "            max_width_px = 0\n",
    "        else:\n",
    "            # Distance transform gives radius, so * 2 for diameter (width)\n",
    "            max_width_px = dist_transform.max() * 2 \n",
    "            \n",
    "        # 5. Convert to Millimeters\n",
    "        width_mm = max_width_px * MM_PER_PIXEL\n",
    "        \n",
    "        # 6. Apply Your Exact Standards\n",
    "        if width_mm <= 0.1:\n",
    "            category = 'light'\n",
    "            count_light += 1\n",
    "        elif width_mm <= 0.3:\n",
    "            # Meaning > 0.1 and <= 0.3\n",
    "            category = 'medium'\n",
    "            count_medium += 1\n",
    "        else:\n",
    "            # Meaning > 0.3\n",
    "            category = 'heavy'\n",
    "            count_heavy += 1\n",
    "            \n",
    "        # 7. Move/Copy File\n",
    "        shutil.copy(img_path, os.path.join(output_base, category, filename))\n",
    "\n",
    "    print(\"\\n--- Sorting Complete ---\")\n",
    "    print(f\"Light  (<= 0.1mm): {count_light}\")\n",
    "    print(f\"Medium (0.1-0.3mm): {count_medium}\")\n",
    "    print(f\"Heavy  (> 0.3mm):  {count_heavy}\")\n",
    "    print(f\"Check folder: {output_base}\")\n",
    "\n",
    "# --- RUN IT ---\n",
    "# Input: Your folder of standardized/preprocessed patches\n",
    "input_dir = 'Positive' \n",
    "output_dir = 'Crack'\n",
    "\n",
    "if os.path.exists(input_dir):\n",
    "    sort_cracks_by_mm(input_dir, output_dir)\n",
    "else:\n",
    "    print(f\"Error: {input_dir} not found. Please run Phase 1 preprocessing first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0fe075a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Counts:\n",
      "  - heavy: 15187\n",
      "  - light: 3039\n",
      "  - medium: 1774\n",
      "  - No Crack: 20000\n",
      "\n",
      "Target count (Smallest Class): 1774\n",
      "All folders will be reduced to 1774 images.\n",
      "Downsampling 'heavy': Removing 13413 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Moving heavy: 100%|██████████| 13413/13413 [00:13<00:00, 1000.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling 'light': Removing 1265 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Moving light: 100%|██████████| 1265/1265 [00:01<00:00, 1244.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 'medium' (already at minimum size).\n",
      "Downsampling 'No Crack': Removing 18226 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Moving No Crack: 100%|██████████| 18226/18226 [00:13<00:00, 1393.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Success! Dataset balanced. Unused images moved to 'dataset_unused_data'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def undersample_dataset(dataset_path, backup_path):\n",
    "    \"\"\"\n",
    "    Reduces the number of images in larger folders to match the smallest folder.\n",
    "    Excess images are moved to 'backup_path' (not deleted permanently).\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(f\"Error: Folder '{dataset_path}' not found.\")\n",
    "        return\n",
    "\n",
    "    # Create backup folder for the discarded images\n",
    "    if not os.path.exists(backup_path):\n",
    "        os.makedirs(backup_path)\n",
    "\n",
    "    # 1. Identify classes and count images\n",
    "    classes = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
    "    class_counts = {}\n",
    "    file_lists = {}\n",
    "    \n",
    "    print(\"Initial Counts:\")\n",
    "    for cls in classes:\n",
    "        cls_dir = os.path.join(dataset_path, cls)\n",
    "        # Get list of valid images\n",
    "        files = [f for f in os.listdir(cls_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        class_counts[cls] = len(files)\n",
    "        file_lists[cls] = files\n",
    "        print(f\"  - {cls}: {len(files)}\")\n",
    "        \n",
    "    # Find the target number (the size of the SMALLEST folder)\n",
    "    target_count = min(class_counts.values())\n",
    "    print(f\"\\nTarget count (Smallest Class): {target_count}\")\n",
    "    print(f\"All folders will be reduced to {target_count} images.\")\n",
    "    \n",
    "    # 2. Undersample larger classes\n",
    "    for cls in classes:\n",
    "        current_count = class_counts[cls]\n",
    "        \n",
    "        if current_count <= target_count:\n",
    "            print(f\"Skipping '{cls}' (already at minimum size).\")\n",
    "            continue\n",
    "            \n",
    "        diff = current_count - target_count\n",
    "        print(f\"Downsampling '{cls}': Removing {diff} images...\")\n",
    "        \n",
    "        # Create a specific backup folder for this class\n",
    "        cls_backup_dir = os.path.join(backup_path, cls)\n",
    "        os.makedirs(cls_backup_dir, exist_ok=True)\n",
    "        \n",
    "        # Randomly choose files to REMOVE\n",
    "        # We assume the files we KEEP are random, so we pick 'diff' files to move away\n",
    "        files_to_remove = random.sample(file_lists[cls], diff)\n",
    "        \n",
    "        for filename in tqdm(files_to_remove, desc=f\"Moving {cls}\"):\n",
    "            src = os.path.join(dataset_path, cls, filename)\n",
    "            dst = os.path.join(cls_backup_dir, filename)\n",
    "            \n",
    "            # Move the file out of the training set\n",
    "            shutil.move(src, dst)\n",
    "            \n",
    "    print(f\"\\nSuccess! Dataset balanced. Unused images moved to '{backup_path}'.\")\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "input_dir = 'Crack'    # Your sorted dataset\n",
    "unused_dir = 'dataset_unused_data' # Where extra images go (Safe storage)\n",
    "\n",
    "undersample_dataset(input_dir, unused_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc3f8ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found classes: ['heavy', 'light', 'medium', 'No Crack']\n",
      "Class 'heavy': 1241 Train, 354 Val, 179 Test\n",
      "Class 'light': 1241 Train, 354 Val, 179 Test\n",
      "Class 'medium': 1241 Train, 354 Val, 179 Test\n",
      "Class 'No Crack': 1241 Train, 354 Val, 179 Test\n",
      "\n",
      "Success! Dataset split created in: dataset_final\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def split_dataset(input_folder, output_folder, split_ratios=(0.7, 0.2, 0.1)):\n",
    "    \"\"\"\n",
    "    Splits a sorted dataset into Train, Validation, and Test sets.\n",
    "    \n",
    "    input_folder:  Path to folder containing 'light', 'medium', 'large'\n",
    "    output_folder: Path where 'train', 'val', 'test' folders will be created\n",
    "    split_ratios:  (Train%, Val%, Test%) - must sum to 1.0\n",
    "    \"\"\"\n",
    "    \n",
    "    if sum(split_ratios) != 1.0:\n",
    "        print(\"Error: Split ratios must sum to 1.0 (e.g., 0.7 + 0.2 + 0.1)\")\n",
    "        return\n",
    "\n",
    "    # Define the classes based on what's in the input folder\n",
    "    classes = [d for d in os.listdir(input_folder) if os.path.isdir(os.path.join(input_folder, d))]\n",
    "    \n",
    "    print(f\"Found classes: {classes}\")\n",
    "    \n",
    "    # Create the output directories\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        for cls in classes:\n",
    "            os.makedirs(os.path.join(output_folder, split, cls), exist_ok=True)\n",
    "            \n",
    "    # Process each class\n",
    "    for cls in classes:\n",
    "        cls_dir = os.path.join(input_folder, cls)\n",
    "        images = [f for f in os.listdir(cls_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        # Shuffle randomly so the split isn't biased\n",
    "        random.shuffle(images)\n",
    "        \n",
    "        # Calculate split indices\n",
    "        n_total = len(images)\n",
    "        n_train = int(n_total * split_ratios[0])\n",
    "        n_val = int(n_total * split_ratios[1])\n",
    "        # The rest goes to test\n",
    "        \n",
    "        train_imgs = images[:n_train]\n",
    "        val_imgs = images[n_train:n_train + n_val]\n",
    "        test_imgs = images[n_train + n_val:]\n",
    "        \n",
    "        print(f\"Class '{cls}': {len(train_imgs)} Train, {len(val_imgs)} Val, {len(test_imgs)} Test\")\n",
    "        \n",
    "        # Helper function to copy files\n",
    "        def copy_files(file_list, destination_split):\n",
    "            for filename in file_list:\n",
    "                src = os.path.join(cls_dir, filename)\n",
    "                dst = os.path.join(output_folder, destination_split, cls, filename)\n",
    "                shutil.copy(src, dst)\n",
    "\n",
    "        # Execute copies\n",
    "        copy_files(train_imgs, 'train')\n",
    "        copy_files(val_imgs, 'val')\n",
    "        copy_files(test_imgs, 'test')\n",
    "\n",
    "    print(f\"\\nSuccess! Dataset split created in: {output_folder}\")\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# The folder created by the Auto-Sorter (contains light/medium/large)\n",
    "source_data_dir = 'Crack' \n",
    "\n",
    "# The NEW folder that will contain train/val/test\n",
    "final_data_dir = 'dataset_final'\n",
    "\n",
    "# Run the split\n",
    "if os.path.exists(source_data_dir):\n",
    "    split_dataset(source_data_dir, final_data_dir)\n",
    "else:\n",
    "    print(f\"Could not find '{source_data_dir}'. Did you run the Auto-Sorter?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c870a0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Data Loaders...\n",
      "Correct Classes Found: ['No Crack', 'heavy', 'light', 'medium']\n",
      "Train Images: 4964 | Val Images: 1416\n",
      "Initializing Hybrid Model (eva02_tiny_patch14_224.mim_in22k)...\n",
      "Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 [Train]: 100%|██████████| 156/156 [00:35<00:00,  4.44it/s, loss=0.0143, acc=0.829]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Result: Train Acc: 0.8288 | Val Acc: 0.8792 | Val Loss: 0.3070\n",
      "--> New Best Model Saved! (0.8792)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 [Train]: 100%|██████████| 156/156 [00:35<00:00,  4.45it/s, loss=0.00917, acc=0.894]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Result: Train Acc: 0.8936 | Val Acc: 0.9040 | Val Loss: 0.2716\n",
      "--> New Best Model Saved! (0.9040)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 [Train]: 100%|██████████| 156/156 [00:35<00:00,  4.44it/s, loss=0.00801, acc=0.906]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Result: Train Acc: 0.9061 | Val Acc: 0.9075 | Val Loss: 0.2545\n",
      "--> New Best Model Saved! (0.9075)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 [Train]: 100%|██████████| 156/156 [00:34<00:00,  4.47it/s, loss=0.00629, acc=0.929]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Result: Train Acc: 0.9289 | Val Acc: 0.9174 | Val Loss: 0.2513\n",
      "--> New Best Model Saved! (0.9174)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 [Train]: 100%|██████████| 156/156 [00:35<00:00,  4.43it/s, loss=0.0062, acc=0.935] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Result: Train Acc: 0.9347 | Val Acc: 0.9131 | Val Loss: 0.2439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 [Train]: 100%|██████████| 156/156 [00:34<00:00,  4.48it/s, loss=0.00614, acc=0.932]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Result: Train Acc: 0.9317 | Val Acc: 0.8390 | Val Loss: 0.4803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 [Train]: 100%|██████████| 156/156 [00:33<00:00,  4.60it/s, loss=0.00458, acc=0.955]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Result: Train Acc: 0.9555 | Val Acc: 0.9047 | Val Loss: 0.3154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 [Train]: 100%|██████████| 156/156 [00:34<00:00,  4.57it/s, loss=0.00306, acc=0.973]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Result: Train Acc: 0.9730 | Val Acc: 0.8771 | Val Loss: 0.4993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 [Train]: 100%|██████████| 156/156 [00:34<00:00,  4.53it/s, loss=0.00289, acc=0.978]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Result: Train Acc: 0.9778 | Val Acc: 0.9188 | Val Loss: 0.3303\n",
      "--> New Best Model Saved! (0.9188)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15 [Train]: 100%|██████████| 156/156 [00:36<00:00,  4.27it/s, loss=0.00196, acc=0.986]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Result: Train Acc: 0.9863 | Val Acc: 0.9230 | Val Loss: 0.3328\n",
      "--> New Best Model Saved! (0.9230)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15 [Train]: 100%|██████████| 156/156 [00:35<00:00,  4.46it/s, loss=0.00163, acc=0.988]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Result: Train Acc: 0.9883 | Val Acc: 0.9280 | Val Loss: 0.3687\n",
      "--> New Best Model Saved! (0.9280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15 [Train]: 100%|██████████| 156/156 [00:34<00:00,  4.50it/s, loss=0.00135, acc=0.99]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Result: Train Acc: 0.9901 | Val Acc: 0.9258 | Val Loss: 0.4137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15 [Train]: 100%|██████████| 156/156 [00:34<00:00,  4.54it/s, loss=0.00123, acc=0.991]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Result: Train Acc: 0.9913 | Val Acc: 0.9287 | Val Loss: 0.3898\n",
      "--> New Best Model Saved! (0.9287)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15 [Train]: 100%|██████████| 156/156 [00:33<00:00,  4.61it/s, loss=0.00133, acc=0.99] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Result: Train Acc: 0.9897 | Val Acc: 0.9237 | Val Loss: 0.3803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15 [Train]: 100%|██████████| 156/156 [00:33<00:00,  4.60it/s, loss=0.00124, acc=0.992]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Result: Train Acc: 0.9921 | Val Acc: 0.9301 | Val Loss: 0.3625\n",
      "--> New Best Model Saved! (0.9301)\n",
      "\n",
      "Training Complete. Best Validation Accuracy: 0.9301\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.ops import deform_conv2d\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==========================================\n",
    "#               CONFIGURATION\n",
    "# ==========================================\n",
    "CONFIG = {\n",
    "    'model_name': 'eva02_tiny_patch14_224.mim_in22k', # Pretrained ViT\n",
    "    'img_size': 224,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 15,\n",
    "    'lr': 1e-4,\n",
    "    'weight_decay': 0.01,\n",
    "    'patience': 5,\n",
    "    'num_classes': 4, # Kaggle usually: Glioma, Meningioma, Pituitary, NoTumor\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'data_dir': 'dataset_final/',  # <--- UPDATE THIS PATH\n",
    "    'checkpoint_path': 'hybrid_eva02_dcnn_best.pth',\n",
    "    'seed': 42,\n",
    "}\n",
    "\n",
    "# ==========================================\n",
    "#           1. UTILITIES & DATA\n",
    "# ==========================================\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def get_dataloaders(data_dir):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((CONFIG['img_size'], CONFIG['img_size'])),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # --- CORRECTED LOADING LOGIC ---\n",
    "    # We point ImageFolder specifically to the 'train' and 'val' subfolders\n",
    "    train_dir = os.path.join(data_dir, 'train')\n",
    "    val_dir = os.path.join(data_dir, 'val')\n",
    "\n",
    "    if not os.path.exists(train_dir):\n",
    "        print(f\"ERROR: Training folder not found at {train_dir}\")\n",
    "        exit()\n",
    "\n",
    "    # Load datasets directly from their respective folders\n",
    "    # No random_split needed because files are already physically separated\n",
    "    train_dataset = datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "    val_dataset = datasets.ImageFolder(root=val_dir, transform=transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n",
    "    \n",
    "    print(f\"Correct Classes Found: {train_dataset.classes}\")\n",
    "    print(f\"Train Images: {len(train_dataset)} | Val Images: {len(val_dataset)}\")\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "# ==========================================\n",
    "#        2. ARCHITECTURE COMPONENTS\n",
    "# ==========================================\n",
    "\n",
    "class LocalCNNBranch(nn.Module):\n",
    "    \"\"\"\n",
    "    A lightweight CNN to extract local textures/edges.\n",
    "    Downsamples 224x224 -> 16x16 to match the ViT patch grid.\n",
    "    \"\"\"\n",
    "    def __init__(self, out_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # 224 -> 112\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            # 112 -> 56\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            # 56 -> 28\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            # 28 -> 14 (Matches Eva02 patch grid 224/16 = 14)\n",
    "            nn.Conv2d(256, out_dim, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_dim),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class CrossAttentionFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Fuses Local CNN features (Query) with Global ViT features (Key/Value).\n",
    "    Handles different sequence lengths between CNN and ViT.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads=4, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(dim, dim, bias=qkv_bias) # For CNN\n",
    "        self.k_proj = nn.Linear(dim, dim, bias=qkv_bias) # For ViT\n",
    "        self.v_proj = nn.Linear(dim, dim, bias=qkv_bias) # For ViT\n",
    "\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x_local, x_global):\n",
    "        # x_local: [Batch, Channels, H, W] (CNN Feature Map)\n",
    "        B, C, H, W = x_local.shape\n",
    "        # Flatten CNN to [Batch, N_local, C] -> This is our Query length\n",
    "        x_local_flat = x_local.flatten(2).transpose(1, 2) \n",
    "        \n",
    "        # x_global: [Batch, N_vit, C] (ViT Tokens) -> This is our Key/Value length\n",
    "        B_v, N_vit, C_v = x_global.shape\n",
    "\n",
    "        # --- Projections ---\n",
    "        # 1. Query (CNN): Uses H*W (196)\n",
    "        q = self.q_proj(x_local_flat).reshape(B, H*W, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        \n",
    "        # 2. Key & Value (ViT): Uses N_vit (256) \n",
    "        # (This was the cause of the error; we now use N_vit instead of H*W)\n",
    "        k = self.k_proj(x_global).reshape(B, N_vit, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        v = self.v_proj(x_global).reshape(B, N_vit, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "\n",
    "        # --- Attention ---\n",
    "        # Matrix multiplication: (Q @ K.T)\n",
    "        # Shape: [Batch, Heads, 196, 256] -> Maps 256 ViT tokens to 196 CNN locations\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "\n",
    "        # --- Weighted Sum ---\n",
    "        # (Attn @ V) -> Shape: [Batch, Heads, 196, Head_Dim]\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, H*W, C)\n",
    "        \n",
    "        # Project back\n",
    "        x = self.proj(x)\n",
    "        \n",
    "        # Residual + Norm\n",
    "        x = self.norm(x + x_local_flat)\n",
    "        \n",
    "        # Reshape back to CNN spatial dimensions [Batch, C, 14, 14]\n",
    "        return x.transpose(1, 2).reshape(B, C, H, W)\n",
    "\n",
    "class DeformableDCNNHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Applies Deformable Convolution to handle irregular tumor boundaries.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Offset predictor: learns WHERE to look\n",
    "        self.offset_conv = nn.Conv2d(dim, 2 * 3 * 3, kernel_size=3, padding=1)\n",
    "        \n",
    "        # The actual weights for the convolution\n",
    "        self.deform_weight = nn.Parameter(torch.Tensor(dim, dim, 3, 3))\n",
    "        nn.init.kaiming_uniform_(self.deform_weight, nonlinearity='relu')\n",
    "        \n",
    "        self.bn = nn.BatchNorm2d(dim)\n",
    "        self.act = nn.GELU()\n",
    "        \n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.classifier = nn.Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [Batch, Dim, 14, 14]\n",
    "        \n",
    "        # 1. Predict Offsets\n",
    "        offsets = self.offset_conv(x)\n",
    "        \n",
    "        # 2. Apply Deformable Conv\n",
    "        x = deform_conv2d(x, offsets, self.deform_weight, padding=1)\n",
    "        x = self.act(self.bn(x))\n",
    "        \n",
    "        # 3. Classify\n",
    "        x = self.global_pool(x).flatten(1)\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "\n",
    "# ==========================================\n",
    "#           3. THE HYBRID MODEL\n",
    "# ==========================================\n",
    "class HybridEva02_DCNN(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, pretrained=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # --- Branch A: Eva-02 Transformer ---\n",
    "        self.vit = timm.create_model(model_name, pretrained=pretrained, num_classes=0)\n",
    "        self.embed_dim = self.vit.num_features \n",
    "        \n",
    "        # --- Branch B: Local CNN ---\n",
    "        self.cnn = LocalCNNBranch(out_dim=self.embed_dim)\n",
    "        \n",
    "        # --- Fusion: Cross Attention ---\n",
    "        self.fusion = CrossAttentionFusion(dim=self.embed_dim)\n",
    "        \n",
    "        # --- Head: Deformable CNN ---\n",
    "        self.head = DeformableDCNNHead(dim=self.embed_dim, num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. Global Features (ViT)\n",
    "        # forward_features returns [Batch, 1+256, Dim]. \n",
    "        # We slice [:, 1:, :] to remove the class token. \n",
    "        # Result: [Batch, 256, Dim]\n",
    "        x_vit = self.vit.forward_features(x)[:, 1:, :] \n",
    "        \n",
    "        # 2. Local Features (CNN)\n",
    "        # Result: [Batch, Dim, 14, 14]\n",
    "        x_cnn = self.cnn(x) \n",
    "        \n",
    "        # 3. Cross Attention Fusion\n",
    "        # Now handles the 256 vs 196 mismatch automatically\n",
    "        x_fused = self.fusion(x_local=x_cnn, x_global=x_vit)\n",
    "        \n",
    "        # 4. Deformable Classification\n",
    "        logits = self.head(x_fused)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# ==========================================\n",
    "#             4. TRAINING LOOP\n",
    "# ==========================================\n",
    "def train_engine():\n",
    "    seed_everything(CONFIG['seed'])\n",
    "    \n",
    "    # Init Data and Model\n",
    "    print(\"Initializing Data Loaders...\")\n",
    "    train_loader, val_loader = get_dataloaders(CONFIG['data_dir'])\n",
    "    \n",
    "    print(f\"Initializing Hybrid Model ({CONFIG['model_name']})...\")\n",
    "    model = HybridEva02_DCNN(CONFIG['model_name'], CONFIG['num_classes']).to(CONFIG['device'])\n",
    "    \n",
    "    # Optimization\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG['lr'], weight_decay=CONFIG['weight_decay'])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2)\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    \n",
    "    print(\"Starting Training...\")\n",
    "    for epoch in range(CONFIG['epochs']):\n",
    "        # --- TRAIN ---\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{CONFIG['epochs']} [Train]\")\n",
    "        for inputs, labels in pbar:\n",
    "            inputs, labels = inputs.to(CONFIG['device']), labels.to(CONFIG['device'])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient Clipping (Stability for Transformers)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({'loss': running_loss/total, 'acc': correct/total})\n",
    "            \n",
    "        train_acc = correct / total\n",
    "        \n",
    "        # --- VALIDATION ---\n",
    "        model.eval()\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(CONFIG['device']), labels.to(CONFIG['device'])\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "                \n",
    "        val_acc = val_correct / val_total\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} Result: Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Save Best\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model.state_dict(), CONFIG['checkpoint_path'])\n",
    "            print(f\"--> New Best Model Saved! ({best_acc:.4f})\")\n",
    "            \n",
    "    print(f\"\\nTraining Complete. Best Validation Accuracy: {best_acc:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5751110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Classes: ['No Crack', 'heavy', 'light', 'medium']\n",
      "Test Images: 716\n",
      "\n",
      "Running Test Evaluation...\n",
      "\n",
      "===============================\n",
      " Test Accuracy: 0.9176\n",
      " Test Loss:     0.4371\n",
      "===============================\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No Crack       1.00      1.00      1.00       179\n",
      "       heavy       0.98      0.91      0.94       179\n",
      "       light       0.90      0.88      0.89       179\n",
      "      medium       0.81      0.88      0.84       179\n",
      "\n",
      "    accuracy                           0.92       716\n",
      "   macro avg       0.92      0.92      0.92       716\n",
      "weighted avg       0.92      0.92      0.92       716\n",
      "\n",
      "Confusion Matrix:\n",
      "[[179   0   0   0]\n",
      " [  0 163   0  16]\n",
      " [  0   0 158  21]\n",
      " [  0   4  18 157]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# Load Test Data\n",
    "# -----------------------------\n",
    "def get_test_loader(test_dir, img_size=224, batch_size=32):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f\"Test Classes: {test_dataset.classes}\")\n",
    "    print(f\"Test Images: {len(test_dataset)}\")\n",
    "\n",
    "    return test_loader, test_dataset.classes\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# Load Model & Evaluate on Test Set\n",
    "# ------------------------------------------\n",
    "def test_model():\n",
    "    device = CONFIG[\"device\"]\n",
    "\n",
    "    # Load Test Loader\n",
    "    test_dir = os.path.join(CONFIG[\"data_dir\"], \"test\")\n",
    "    test_loader, class_names = get_test_loader(test_dir, CONFIG[\"img_size\"], CONFIG[\"batch_size\"])\n",
    "\n",
    "    # Load Model\n",
    "    model = HybridEva02_DCNN(CONFIG['model_name'], CONFIG['num_classes'])\n",
    "    model.load_state_dict(torch.load(CONFIG[\"checkpoint_path\"], map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    print(\"\\nRunning Test Evaluation...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "    test_acc = correct / total\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "\n",
    "    print(f\"\\n===============================\")\n",
    "    print(f\" Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\" Test Loss:     {avg_loss:.4f}\")\n",
    "    print(\"===============================\\n\")\n",
    "\n",
    "    # Classification Report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # ------------------------------------\n",
    "    # 📌 Seaborn Heatmap Confusion Matrix\n",
    "    # ------------------------------------\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm,\n",
    "                annot=True,\n",
    "                fmt=\"d\",\n",
    "                cmap=\"Blues\",\n",
    "                xticklabels=class_names,\n",
    "                yticklabels=class_names)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix - HybridEva02 DCNN\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Run Test\n",
    "test_model()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
